{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e2141ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ef51e070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = dict(\n",
      "    type='MaskRCNN',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        depth=50,\n",
      "        num_stages=4,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        frozen_stages=1,\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        norm_eval=True,\n",
      "        style='pytorch',\n",
      "        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[256, 512, 1024, 2048],\n",
      "        out_channels=256,\n",
      "        num_outs=5),\n",
      "    rpn_head=dict(\n",
      "        type='RPNHead',\n",
      "        in_channels=256,\n",
      "        feat_channels=256,\n",
      "        anchor_generator=dict(\n",
      "            type='AnchorGenerator',\n",
      "            scales=[8],\n",
      "            ratios=[0.5, 1.0, 2.0],\n",
      "            strides=[4, 8, 16, 32, 64]),\n",
      "        bbox_coder=dict(\n",
      "            type='DeltaXYWHBBoxCoder',\n",
      "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "        loss_cls=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "    roi_head=dict(\n",
      "        type='StandardRoIHead',\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        bbox_head=dict(\n",
      "            type='Shared2FCBBoxHead',\n",
      "            in_channels=256,\n",
      "            fc_out_channels=1024,\n",
      "            roi_feat_size=7,\n",
      "            num_classes=80,\n",
      "            bbox_coder=dict(\n",
      "                type='DeltaXYWHBBoxCoder',\n",
      "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "            reg_class_agnostic=False,\n",
      "            loss_cls=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n",
      "            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "        mask_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        mask_head=dict(\n",
      "            type='FCNMaskHead',\n",
      "            num_convs=4,\n",
      "            in_channels=256,\n",
      "            conv_out_channels=256,\n",
      "            num_classes=80,\n",
      "            loss_mask=dict(\n",
      "                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),\n",
      "    train_cfg=dict(\n",
      "        rpn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.7,\n",
      "                neg_iou_thr=0.3,\n",
      "                min_pos_iou=0.3,\n",
      "                match_low_quality=True,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=False),\n",
      "            allowed_border=-1,\n",
      "            pos_weight=-1,\n",
      "            debug=False),\n",
      "        rpn_proposal=dict(\n",
      "            nms_pre=2000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.5,\n",
      "                neg_iou_thr=0.5,\n",
      "                min_pos_iou=0.5,\n",
      "                match_low_quality=True,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=512,\n",
      "                pos_fraction=0.25,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=True),\n",
      "            mask_size=28,\n",
      "            pos_weight=-1,\n",
      "            debug=False)),\n",
      "    test_cfg=dict(\n",
      "        rpn=dict(\n",
      "            nms_pre=1000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            score_thr=0.05,\n",
      "            nms=dict(type='nms', iou_threshold=0.5),\n",
      "            max_per_img=100,\n",
      "            mask_thr_binary=0.5)))\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = 'data/coco/'\n",
      "backend_args = None\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile', backend_args=None),\n",
      "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "    dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n",
      "    dict(type='RandomFlip', prob=0.5),\n",
      "    dict(type='PackDetInputs')\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile', backend_args=None),\n",
      "    dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n",
      "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "    dict(\n",
      "        type='PackDetInputs',\n",
      "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n",
      "                   'scale_factor'))\n",
      "]\n",
      "train_dataloader = dict(\n",
      "    batch_size=2,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
      "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
      "    dataset=dict(\n",
      "        type='CocoDataset',\n",
      "        data_root='data/coco/',\n",
      "        ann_file='annotations/instances_train2017.json',\n",
      "        data_prefix=dict(img='train2017/'),\n",
      "        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile', backend_args=None),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "            dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n",
      "            dict(type='RandomFlip', prob=0.5),\n",
      "            dict(type='PackDetInputs')\n",
      "        ],\n",
      "        backend_args=None))\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    drop_last=False,\n",
      "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
      "    dataset=dict(\n",
      "        type='CocoDataset',\n",
      "        data_root='data/coco/',\n",
      "        ann_file='annotations/instances_val2017.json',\n",
      "        data_prefix=dict(img='val2017/'),\n",
      "        test_mode=True,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile', backend_args=None),\n",
      "            dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "            dict(\n",
      "                type='PackDetInputs',\n",
      "                meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n",
      "                           'scale_factor'))\n",
      "        ],\n",
      "        backend_args=None))\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    drop_last=False,\n",
      "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
      "    dataset=dict(\n",
      "        type='CocoDataset',\n",
      "        data_root='data/coco/',\n",
      "        ann_file='annotations/instances_val2017.json',\n",
      "        data_prefix=dict(img='val2017/'),\n",
      "        test_mode=True,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile', backend_args=None),\n",
      "            dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "            dict(\n",
      "                type='PackDetInputs',\n",
      "                meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n",
      "                           'scale_factor'))\n",
      "        ],\n",
      "        backend_args=None))\n",
      "val_evaluator = dict(\n",
      "    type='CocoMetric',\n",
      "    ann_file='data/coco/annotations/instances_val2017.json',\n",
      "    metric=['bbox', 'segm'],\n",
      "    format_only=False,\n",
      "    backend_args=None)\n",
      "test_evaluator = dict(\n",
      "    type='CocoMetric',\n",
      "    ann_file='data/coco/annotations/instances_val2017.json',\n",
      "    metric=['bbox', 'segm'],\n",
      "    format_only=False,\n",
      "    backend_args=None)\n",
      "train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=12, val_interval=1)\n",
      "val_cfg = dict(type='ValLoop')\n",
      "test_cfg = dict(type='TestLoop')\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        type='LinearLR', start_factor=0.001, by_epoch=False, begin=0, end=500),\n",
      "    dict(\n",
      "        type='MultiStepLR',\n",
      "        begin=0,\n",
      "        end=12,\n",
      "        by_epoch=True,\n",
      "        milestones=[8, 11],\n",
      "        gamma=0.1)\n",
      "]\n",
      "optim_wrapper = dict(\n",
      "    type='OptimWrapper',\n",
      "    optimizer=dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001))\n",
      "auto_scale_lr = dict(enable=False, base_batch_size=16)\n",
      "default_scope = 'mmdet'\n",
      "default_hooks = dict(\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    logger=dict(type='LoggerHook', interval=50),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    checkpoint=dict(type='CheckpointHook', interval=1),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    visualization=dict(type='DetVisualizationHook'))\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
      "    dist_cfg=dict(backend='nccl'))\n",
      "vis_backends = [dict(type='LocalVisBackend')]\n",
      "visualizer = dict(\n",
      "    type='DetLocalVisualizer',\n",
      "    vis_backends=[dict(type='LocalVisBackend')],\n",
      "    name='visualizer')\n",
      "log_processor = dict(type='LogProcessor', window_size=50, by_epoch=True)\n",
      "log_level = 'INFO'\n",
      "load_from = '/home/host/mmdetection/checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth'\n",
      "resume = False\n",
      "num_classes = 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv import Config\n",
    "from mmdet.apis import set_random_seed\n",
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.apis import train_detector, init_detector, inference_detector\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "\n",
    "# https://paperswithcode.com/lib/mmdetection - веса для моделей брал тут\n",
    "# https://github.com/open-mmlab/mmdetection/tree/main/configs/faster_rcnn\n",
    "\n",
    "#cfg = Config.fromfile('/home/host/mmdetection/configs/detectors/cascade-rcnn_r50-rfp_1x_coco.py')\n",
    "#cfg.load_from = '/home/host/mmdetection/checkpoints/cascade_rcnn_r50_rfp_1x_coco-8cf51bfd.pth'\n",
    "\n",
    "cfg = Config.fromfile('/home/host/mmdetection/configs/mask_rcnn/mask-rcnn_r50_fpn_1x_coco.py')\n",
    "cfg.load_from = '/home/host/mmdetection/checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth'\n",
    "\n",
    "# удаляем все на что ругается mmdetection\n",
    "m_type = cfg.model.type\n",
    "m_backbone = cfg.model.backbone\n",
    "m_neck = cfg.model.neck\n",
    "m_rpn_head = cfg.model.rpn_head\n",
    "m_roi_head = cfg.model.roi_head\n",
    "m_train_cfg = cfg.model.train_cfg\n",
    "m_test_cfg = cfg.model.test_cfg\n",
    "\n",
    "\n",
    "\n",
    "cfg.model = dict(type=m_type, backbone=m_backbone, neck=m_neck,\n",
    "                 rpn_head=m_rpn_head, roi_head=m_roi_head, train_cfg=m_train_cfg, test_cfg=m_test_cfg)\n",
    "cfg.num_classes = 1\n",
    "\n",
    "print(cfg.pretty_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2a01213e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CocoDataset: CustomDataset.__init__() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmcv/utils/registry.py:69\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Normal TypeError does not print class name.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: CustomDataset.__init__() got an unexpected keyword argument 'batch_size'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m clear_output()\n\u001b[1;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m build_detector(cfg\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m---> 42\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [\u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39mCLASSES \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mCLASSES\n\u001b[1;32m     45\u001b[0m train_detector(model, datasets, cfg, distributed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mmdet/datasets/builder.py:82\u001b[0m, in \u001b[0;36mbuild_dataset\u001b[0;34m(cfg, default_args)\u001b[0m\n\u001b[1;32m     80\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m _concat_dataset(cfg, default_args)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_from_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATASETS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/mmcv/utils/registry.py:72\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj_cls(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Normal TypeError does not print class name.\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: CocoDataset: CustomDataset.__init__() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "DATASET_TYPE = 'CocoDataset'\n",
    "PREFIX = '/home/host/Documents/JupyterProjects/Test_ML_DBA_1/dataset/'\n",
    "cfg.dataset_type = DATASET_TYPE\n",
    "cfg.classes = ('support')\n",
    "#cfg.batch_size = 64\n",
    "#cfg.model.num_classes = 1\n",
    "\n",
    "\n",
    "cfg.train_dataloader.dataset.data_prefix = dict(img='images/')\n",
    "cfg.train_dataloader.dataset.data_root = PREFIX + 'train/'\n",
    "cfg.train_dataloader.dataset.ann_file = 'train.json'\n",
    "cfg.train_dataloader.dataset.type = DATASET_TYPE\n",
    "\n",
    "cfg.test_dataloader.dataset.data_prefix = dict(img='images/')\n",
    "cfg.test_dataloader.dataset.data_root = PREFIX + 'test/'\n",
    "cfg.test_dataloader.dataset.ann_file = 'test.json'\n",
    "cfg.test_dataloader.dataset.type = DATASET_TYPE\n",
    "\n",
    "cfg.optimizer = dict(type='Adam', lr=0.0003, weight_decay=0.0001)\n",
    "#cfg.lr_config.warmup = None\n",
    "#cfg.lr_config.policy = 'step'\n",
    "#cfg.log_config.interval = 100\n",
    "\n",
    "cfg.test_evaluator.metric = 'bbox'\n",
    "\n",
    "#cfg.evaluation.interval = 5\n",
    "#cfg.checkpoint_config.interval = 5\n",
    "\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "\n",
    "cfg.work_dir = \"/home/host/Documents/JupyterProjects/Test_ML_DBA_1/out\"\n",
    "\n",
    "#cfg.runner.max_epochs = 10\n",
    "cfg.total_epochs = 10\n",
    "\n",
    "\n",
    "clear_output()\n",
    "model = build_detector(cfg.model)\n",
    "datasets = [build_dataset(cfg.train_dataloader)]\n",
    "model.CLASSES = datasets[0].CLASSES\n",
    "\n",
    "train_detector(model, datasets, cfg, distributed=False, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "322055f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 49\u001b[0m\n\u001b[1;32m     22\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     23\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     24\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         )\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     37\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     38\u001b[0m     train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     39\u001b[0m         dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m         classes\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[1;32m     47\u001b[0m         ann_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/test/test.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 49\u001b[0m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtest_cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "dataset_type = 'CocoDataset'\n",
    "classes = ('support')\n",
    "data_root='/home/host/Documents/JupyterProjects/Test_ML_DBA_1/dataset/'\n",
    "\n",
    "train_dataloader = dict(\n",
    "    batch_size=2,\n",
    "    num_workers=2,\n",
    "    dataset=dict(\n",
    "        type=dataset_type,\n",
    "        metainfo=dict(classes=classes),\n",
    "        data_root=data_root,\n",
    "        ann_file='train/train.json',\n",
    "        data_prefix=dict(img='train/images')\n",
    "        )\n",
    "    )\n",
    "\n",
    "test_dataloader = dict(\n",
    "    batch_size=2,\n",
    "    num_workers=2,\n",
    "    dataset=dict(\n",
    "        type=dataset_type,\n",
    "        test_mode=True,\n",
    "        metainfo=dict(classes=classes),\n",
    "        data_root=data_root,\n",
    "        ann_file='test/test.json',\n",
    "        data_prefix=dict(img='test/images')\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "model = dict(\n",
    "    train=dict(\n",
    "        dataset=dict(\n",
    "            img_prefix='dataset/train/',\n",
    "            classes=classes,\n",
    "            ann_file='dataset/train/train.json')\n",
    "    ),\n",
    "    test=dict(\n",
    "        img_prefix='dataset/test/',\n",
    "        classes=classes,\n",
    "        ann_file='dataset/test/test.json'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = mmcv.load('/home/host/Documents/JupyterProjects/Test_ML_DBA_1/dataset/train/train.json')\n",
    "data_test = mmcv.load('/home/host/Documents/JupyterProjects/Test_ML_DBA_1/dataset/test/test.json')\n",
    "\n",
    "\n",
    "\n",
    "annotation = {\n",
    "        \"filename\": filename,\n",
    "        \"height\": 365,\n",
    "        \"width\": 650,\n",
    "        \"id\": filename.split('.')[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab150b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@DATASETS.register_module()\n",
    "class MyDataset(CustomDataset):\n",
    "\n",
    "    CLASSES = classes\n",
    "\n",
    "    def load_data_list(self, ann_file):\n",
    "        image_list = mmcv.list_from_file(self.ann_file)\n",
    "\n",
    "        data_infos = []\n",
    "        for image_id in image_list:\n",
    "            filename = f'{self.data_prefix}/{image_id}.jpg'\n",
    "            \n",
    "            image = mmcv.imread(filename)\n",
    "            height, width = image.shape[:2]\n",
    "            \n",
    "            data_info = dict(filename=f'{image_id}.jpeg', width=width, height=height)\n",
    "            \n",
    "            #img_shape = ann_list[i + 2].split(' ')\n",
    "            #width = int(img_shape[0])\n",
    "            #height = int(img_shape[1])\n",
    "            #bbox_number = int(ann_list[i + 3])\n",
    "\n",
    "            instances = []\n",
    "            for anns in ann_list[i + 4:i + 4 + bbox_number]:\n",
    "                instance = {}\n",
    "                instance['bbox'] = [float(ann) for ann in anns.split(' ')[:4]]\n",
    "                instance['bbox_label']=int(anns[4])\n",
    "                instances.append(instance)\n",
    "\n",
    "            data_infos.append(\n",
    "                dict(\n",
    "                    img_path=ann_list[i + 1],\n",
    "                    img_id=i,\n",
    "                    width=width,\n",
    "                    height=height,\n",
    "                    instances=instances\n",
    "                ))\n",
    "\n",
    "        return data_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "config_file = Config.fromfile('/home/host/mmdetection/configs/mask_rcnn/mask-rcnn_r50_fpn_1x_coco.py')\n",
    "#checkpoint_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39475f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis import init_detector, inference_detector\n",
    "config_file = '/home/host/mmdetection/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco.py'\n",
    "# Setup a checkpoint file to load\n",
    "checkpoint = '/home/host/mmdetection/checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_detector(config_file, checkpoint_file, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539dc387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
